{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ASTR 324, University of Washington\n",
    "# Week 2: Introduction to Probability & Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Resources for this notebook include:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapters 3 and 4.  \n",
    "- [Gordon Richard's notebooks](https://github.com/gtrichards/PHYS_T480)\n",
    "- random contributions from a large number of colleagues (e.g. Jake VanderPlas, Andy Connolly)\n",
    "\n",
    "##### Suggested supplemental background reading:\n",
    "\n",
    "[David Hogg: \"Data analysis recipes: Probability calculus for inference\"](https://arxiv.org/abs/1205.4446)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Learning goals for Week 2 (mostly based on Chapter 3 material): \n",
    "\n",
    "- Mathematics of probability (notation, definitions, conditional probability, Bayes Rule)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goals of Science\n",
    "\n",
    "What are our goals, as scientists?\n",
    "\n",
    "1. Describe the world around us.\n",
    "1. Predict outcomes.\n",
    "1. Explain the observations and predictions.\n",
    "\n",
    "My goal here is to introduce you to techniques allowing you to quantitatively, reproducibly, and objectivelly reach those goals -- *build knowledge* -- from collected observations and conducted experiments (data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Science and probability\n",
    "\n",
    "* Most science is fundamentally *inductive*: based on a _limited_ set of specific observations, we attempt to generalize and infer the underlying natural laws (rules of cause and effect) that govern the observed phenomena.\n",
    "\n",
    "* This generalization naturally leads to uncertainty: e.g., as we haven't observed or tested all possible outcomes, it's always possible our model (theory) may fail under some circumstances.\n",
    "\n",
    "* That is fine. Science advances by rejecting (_falsifying_) theories when data is found that does not fit them. How do we know if the discrepancy is significant enough for a theory to be rejected?\n",
    "\n",
    "* To be able to objectively and reproducibly reason about these questions, we need to quantify uncertainty. It's matematical expression is that of *probability*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Mathematics of Probability\n",
    "\n",
    "We distinguish between ***events*** (e.g. \"we rolled an even number\") and a set of ***outcomes*** favorable to those events (e.g., \"2, 4, 6\").\n",
    "\n",
    "We'll adopt a working definition of probability $P(A)$ as **the ratio of the number of outcomes favorable to event A over the number of total possible outcomes**. If the outcomes are continuous, $p(x) dx$ will denote the probability of events in an infinitesimally small range around $x$.\n",
    "\n",
    "This definition satisfies the Axioms of Probability (Kolmogorov, 1933):\n",
    "1. $p(A)$ must be $\\ge 0$ for all $A$ and,\n",
    "2. The sum of probabilities of all events (or the integral of the probability density function) must be unity.\n",
    "3. If $A$ and $B$ are disjoint (independent) events, the probability of _either_ occuring is $p(A \\,{\\rm or}\\, B) = P(A) + P(B)$\n",
    "\n",
    "Note: any function satisfying Kolmogorov's axioms can be considered to describe probability, irrespective of our interpretation. That will become important later when we re-interpret probability as the degree of belief (knowledge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are \"Events\"?\n",
    "\n",
    "Events can be nearly anything:\n",
    "\n",
    "* The properties of numbers after a roll of a dice\n",
    "* The numbers on consecutive rolls of a dice\n",
    "* Measuring a particular value of position in the x direction\n",
    "* Measuring a value of position in the x-y plane (counts as two \"events\")!\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilities of Multiple Events\n",
    "\n",
    "Nearly all of our work will be about probabilities of multiple events and their relationships.\n",
    "\n",
    "Events and their relationships can be graphically visualized with Venn diagrams. If we have two events, $A$ and $B$, the possible combinations are illustrated by the following figure:\n",
    "![Figure 3.1](http://www.astroml.org/_images/fig_prob_sum_1.png)\n",
    "\n",
    "$A \\cup B$ is the *union* of sets $A$ and $B$. $A \\cap B$ is the *intersection* of sets $A$ and $B$. Nearly all of our work will be about probabilities of multiple events and their relationships.\n",
    "\n",
    "\n",
    "The probability that *either* $A$ or $B$ will happen (which could include both) is the *union*, given by\n",
    "\n",
    "$$p(A \\cup B) = p(A) + p(B) - p(A \\cap B)$$\n",
    "\n",
    "While this can be formally proven from Kolmogorov's axioms, the figure above makes it clear why the last term is necessary.  Since $A$ and $B$ overlap, we are double-counting the outcomes where *both* $A$ and $B$ happen, so we have to subtract this out.\n",
    "\n",
    "The last term $p(A \\cap B)$ is known as the **joint probability** of A and B: the probability A and B will both occur simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example:\n",
    "\n",
    "    A = rolled > 3, B = the number rolled was an even number\n",
    "    \n",
    "    P(greater than 3 or even) = P(> 3) + P(is even) - P(> 3 and even) = 3/6 + 3/6 - 2/6 = 4/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional Probability\n",
    "\n",
    "The probability that *both* $A$ and $B$ will happen, $p(A \\cap B)$, can be written as:\n",
    "\n",
    "$$p(A \\cap B) = p(A \\mid B)\\, p(B) = p(B \\mid A)\\, p(A)$$\n",
    "\n",
    "where $p(A \\mid B)$ is the the **conditional probability** of $A$ occuring *given that* $B$ has occured. This is the **definition** of conditional probability.\n",
    "\n",
    "In words: *\"The probability that both A and B have occured is equal to the probability B has occured times the probability that A will occur if B occured\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example:\n",
    "\n",
    "    A = rolled > 3, B = the number rolled was an even number\n",
    "    \n",
    "    P(greater than 3 and even) = P(> 3|if even)    * P(is even)\n",
    "\n",
    "Working it out:\n",
    "\n",
    "    - All outcomes: 1 2 3 4 5 6\n",
    "    - rolled an even number: 2 4 6\n",
    "        P(is even) = count(2 4 6) / count (1 2 3 4 5 6) = 3/6\n",
    "    - number is > 3, if even: 4 6\n",
    "        P(> 3 | if even)         = count(4 6)   / count (2 4 6)       = 2/3\n",
    "    \n",
    "    P(greater than 3 and even) = 2/3 * 3/6 = 1/3\n",
    "    \n",
    "\"By hand\":\n",
    "\n",
    "    count(4 6) / count(1 2 3 4 5 6) = 2/6 = 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notation\n",
    "\n",
    "Different people and textbooks use different notation for joint probability. The following all mean the same thing:\n",
    "\n",
    "$$p(A \\cap B) = p(A,B) = p(AB) = p(A \\,{\\rm and}\\, B)$$\n",
    "\n",
    "From now on, **we will use the comma notation to be consistent with the textbook**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probability of independent events\n",
    "\n",
    "The following is *always* true (it's the definition of conditional probability):\n",
    "\n",
    "$$p(A,B) = p(A \\mid B) \\, p(B) = p(B \\mid A) \\, p(A)$$\n",
    "\n",
    "However, if $A$ and $B$ are ***independent*** then the above simplifies to:\n",
    "\n",
    "$$p(A,B) = p(A)\\, p(B)$$\n",
    "\n",
    "Events $A$ and $B$ are independent if the outcome of $B$ does not change the possible outcomes (or their probabilities) for $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example:\n",
    "\n",
    "Let's roll the dice twice. What is the probability of obtaining 4 in the second roll (event \"A\"), if we obtained 5 in the first roll (event \"B\")?\n",
    "\n",
    "    p(second is 4, first is 5) = p(second is 4 | first is 5) * p(first is 5)\n",
    "                               = p(second is 4) * p(first is 5)\n",
    "                               = 1/6 * 1/6 = 1/36\n",
    "\n",
    "In other words, ***knowing A happened tells us nothing about whether B happened (or will happen), and vice versa***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example:\n",
    "\n",
    "If you have a bag with 5 marbles, 3 yellow and 2 blue and you want to know the probability of picking 2 yellow marbles in a row, that would be\n",
    "\n",
    "$$p(Y_1, Y_2) = p(Y_1)\\, p(Y_2 \\mid Y_1).$$\n",
    "\n",
    "$p(Y_1) = \\frac{3}{5}$ since you have an equally likely chance of drawing any of the 5 marbles.\n",
    "\n",
    "If you did not put the first marble back in the back after drawing it (sampling *without* \"replacement\"), then the probability\n",
    "$p(Y_2 \\mid Y_1) = \\frac{2}{4}$, so that\n",
    "$$p(Y_1,Y_2) = \\frac{3}{5}\\frac{2}{4} = \\frac{3}{10}.$$\n",
    "\n",
    "But if you put the first marble back, then\n",
    "$p(Y_2 \\mid Y_1) = \\frac{3}{5} = p(Y_2)$, so that \n",
    "$$p(Y_1,Y_2) = \\frac{3}{5}\\frac{3}{5} = \\frac{9}{25}.$$\n",
    "\n",
    "In the first case $A$ and $B$ (or rather $Y_1$ and $Y_2$) are *not* independent, whereas in the second case they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example:\n",
    "\n",
    "     John is successful and John is a Libra.\n",
    "\n",
    "Also, ***knowing A happened tells us nothing about whether B happened (or will happen), and vice versa***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Law of Total Probability (Theorem)\n",
    "\n",
    "![total probability](figures/total-probability-2.png)\n",
    "\n",
    "If events $B_i$ are independent and their union is the set of all possible outcomes, then the **law of total probability** says that:\n",
    "\n",
    "$$p(A) = \\sum_ip(A, B_i) = \\sum_ip(A \\mid B_i)\\, p(B_i)$$\n",
    "\n",
    "What is this? This is **the probability A will occur irrespective of which the set of events B occurs**. This is an *extremely important* concept (known as \"marginal probability\" in the continuous case), as it allows us to ignore uninteresting dependencies in the data, or apply it to cases when they're unknown."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example:\n",
    "\n",
    "Four different studies measured the abundance of planetary systems around main sequence stars of different spectral types (&lt;F, G, K, &gt;M). What is the probability a randomly chosen (== typical) main sequence star will harbor a planetary system?\n",
    "\n",
    "|Spectral type|Fraction with systems|Fraction of MS stars with this spectral type|\n",
    "| --- | ---- | ---  |\n",
    "| < F | 0.1  | 0.1  |\n",
    "|   G | 0.3  | 0.15 |\n",
    "|   K | 0.45 | 0.25 |\n",
    "| > M | 0.7  | 0.5  |\n",
    "\n",
    "The last two columns are $P(A|B_i)$ and $P(B_i)$, respectivelly. Applying the law of total probabilities, we find that the chance a randomly chosen main sequence star harbors a planetary system is:\n",
    "\n",
    "$$ P(A) = 0.01 + 0.045 + 0.1125 + 0.35 =  0.5175 $$\n",
    "\n",
    "or 51.75%.\n",
    "\n",
    "(note: the numbers in the table above are for illustration only, and do not correspond to actual observed rates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous case\n",
    "\n",
    "All of this generalizes to continuous processes (i.e, where events are not discrete -- say, a set of outcomes of a roll of a dice -- but continouos -- say, the result of measurement of the position of a star). The generalization is usually as simple as $\\sum_i \\rightarrow \\int dx$.\n",
    "\n",
    "### Probability density\n",
    "In the continous case, we speak of the _probability density_, $p(x)$ of the outcome being $x$. The _probability_ makes sense only when integrated over an interval, i.e.:\n",
    "\n",
    "$$ p(x_0 < x < x_1) = \\int_{x_0}^{x_1} p(x) dx$$\n",
    "\n",
    "is the probability of the value of $x$ being in some interval $(x_0, x_1)$.\n",
    "\n",
    "Example: the probability a star's brightness is $x$.\n",
    "\n",
    "### Joint probability\n",
    "\n",
    "Similarly, the joint probability of two outcomes being $x$ and $y$ is given by:\n",
    "\n",
    "$$ p(x_0 < x < x_1, y_0 < y < y_1) = \\int_{x_0}^{x_1} \\int_{y_0}^{y_1} p(x, y)\\, dx dy$$\n",
    "\n",
    "Example: the probability a star is at position $(ra, dec)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Marginal probability\n",
    "\n",
    "The law of total probability also transfers over:\n",
    "\n",
    "$$p(x) = \\int p(x|y) p(y) dy,$$\n",
    "\n",
    "Given the probability of $x$ and $y$ occurring can be written in terms of ***conditional probability*** as:\n",
    "\n",
    "$$p(x,y) = p(x \\mid y)\\, p(y) = p(y \\mid x)\\, p(x)$$\n",
    "\n",
    "this is also equal to:\n",
    "\n",
    "$$p(x) = \\int p(x,y)dy,$$\n",
    "\n",
    "which is how we commonly define the ***marginal probability***: the ***probability of $x$ occurring irrespective of the value of $y$***. This is essentially projecting on to one axis (integrating over the other axis, see the figure in the Notebook, below).\n",
    "\n",
    "Again, this is just the law of total probability, but for continous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Marginal vs. contitional probability distributions\n",
    "\n",
    "In the following figure, we have a 2-D distribution in $x-y$ parameter space.  Here $x$ and $y$ are *not* independent as, once you pick a $y$, your values of $x$ are constrained.\n",
    "\n",
    "The *marginal* distributions are shown on the left and bottom sides of the left panel.  As the equation above says, this is just the integral along the $x$ direction for a given $y$ (left side panel) or the integral along the $y$ direction for a given $x$ (bottom panel).  \n",
    "\n",
    "The three panels on the right show the *conditional* probability (of $x$) for three $y$ values:\n",
    "\n",
    "$$p(x|y=y_0)$$\n",
    "\n",
    "These are just \"slices\" through the 2-D distribution.\n",
    "\n",
    "<img src=\"http://www.astroml.org/_images/fig_conditional_probability_1.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Need more help with this?  Try watching some Khan Academy videos and working through the exercises:\n",
    "[https://www.khanacademy.org/math/probability/probability-geometry](https://www.khanacademy.org/math/probability/probability-geometry)\n",
    "\n",
    "[https://www.khanacademy.org/math/precalculus/prob-comb](https://www.khanacademy.org/math/precalculus/prob-comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Starting with conditional probability:\n",
    "\n",
    "$$p(x \\mid y)\\,p(y) = p(y \\mid x)\\,p(x)$$\n",
    "\n",
    "we can write:\n",
    "\n",
    "$$p(y \\mid x) = \\frac{p(x \\mid y)\\,p(y)}{p(x)} = \\frac{p(x \\mid y)\\,p(y)}{\\int p(x \\mid y)\\,p(y)\\,dy}$$\n",
    "\n",
    "which in words says that\n",
    "\n",
    "> the (conditional) probability of $y$ given $x$ is just the (conditional) probability of $x$ given $y$ times the (marginal) probability of $y$ divided by the (marginal) probability of $x$, where the latter is just the integral of the numerator.\n",
    "\n",
    "This is the **Bayes' rule** (a.k.a, Bayes' Theorem). It tells us **how to express $p(y|x)$ (something we'd like to know) in terms of $p(x \\mid y)$ (something we can compute).**\n",
    "\n",
    "Bayes' rule itself isn't at all controversial, though its application can be as we'll discuss later in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: LEGOs \n",
    "\n",
    "An example with LEGOs (it's awesome!):\n",
    "[https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego](https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Monty Hall Problem\n",
    "\n",
    "You are playing a game show and are shown 2 doors.  One has a car behind it, the other a goat.  What are your chances of picking the door with the car?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK, now there are 3 doors: one with a car, two with goats.  The game show host asks you to pick a door, but not to open it yet.  Then the host opens one of the other two doors (that you did not pick), making sure to select one with a goat.  The host offers you the opportunity to switch doors.  Do you switch?\n",
    "<br><br><br>\n",
    "<center> \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Monty_open_door.svg/1280px-Monty_open_door.svg.png\" width=800>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applying the Bayes' Rule\n",
    "\n",
    "Let's apply Bayes' rule:\n",
    "\n",
    "$$p({\\rm car\\,behind\\,X}\\mid{\\rm opened\\,\\#3}) = \\frac{p({\\rm opened\\,\\#3}\\mid{\\rm car\\,behind\\,X})\\,p({\\rm car\\,behind\\,X})}{\\sum_{X=1}^3 p({\\rm opened\\,\\#3}\\mid{\\rm car\\,behind\\,X})\\,p({\\rm car\\,behind\\,X})}$$\n",
    "\n",
    "for each door (denoted X, above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assume we picked door \\#1. Therefore, Monty can choose to open door 2 or 3, noting he'll always choose the one with the car. Let's assume he opened door \\#3.\n",
    "\n",
    "Evaluate the Bayes rule for all three options:\n",
    "\n",
    "```\n",
    "   p(car behind 1 | Monty opened 3) = 1/2 * 1/3    *    1/norm\n",
    "   p(car behind 2 | Monty opened 3) = 1   * 1/3    *    1/norm\n",
    "   p(car behind 3 | Monty opened 3) = 0   * 1/3    *    1/norm\n",
    "```\n",
    "\n",
    "So by switching our choice we increase our chance of winning the car by 2x! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another way to evaluate this -- what is the probability for the car to be behind our door?\n",
    "\n",
    "$$p(1{\\rm st \\; choice}) = 1/3$$\n",
    "\n",
    "And what is the probability that the car is not behind our choice?\n",
    "\n",
    "$$p(other) = 2/3$$\n",
    "\n",
    "Once the host opens one of the two other door, $p({\\rm other}) = 2/3$ does not change even though there's only one door left to choose from. Therefore, ***switching doubles your chances!***\n",
    "\n",
    "More info: https://betterexplained.com/articles/understanding-the-monty-hall-problem/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### N-door version\n",
    "\n",
    "For $N$ choices, revealing $N-2$ \"answers\" doesn't change the probability of your choice being correct.  It is still $\\frac{1}{N}$.  But it *does* change the probability of the *other* remaining choice being correct by $N-1$, which then becomes $\\frac{N-1}{N}$. Therefore, by switching, you increase your chance of winning by a factor of (N-1). \n",
    "\n",
    "In the 3-door example, you double your chance of winning (from 1/3 to 2/3). Think about a case with a thousand doors, where 998 are opened after you've made your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Contingency Table\n",
    "\n",
    "We can also use Bayes' rule to learn something about false positives and false negatives.\n",
    "\n",
    "Let's say that we have a test for a disease.  The test can be positive ($T=1$) or negative ($T=0$) and one can either have the disease ($D=1$) or not ($D=0$).  So, there are 4 possible combinations:\n",
    "\n",
    "$$T=0; D=0 \\;\\;\\;  {\\rm true \\; negative}$$\n",
    "$$T=0; D=1 \\;\\;\\; {\\rm false \\; negative}$$\n",
    "$$T=1; D=0 \\;\\;\\; {\\rm false \\; positive}$$\n",
    "$$T=1; D=1 \\;\\;\\; {\\rm true \\; positive}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All else being equal, you have a 50% chance of being misdiagnosed.  Not good!  But the probability of disease and the accuracy of the test presumably are not random.\n",
    "\n",
    "If the rates of false positive and false negative are:\n",
    "\n",
    "$$p(T=1|D=0) = \\epsilon_{\\rm FP}$$\n",
    "$$p(T=0|D=1) = \\epsilon_{\\rm FN}$$\n",
    "\n",
    "then the true positive and true negative rates are just:\n",
    "\n",
    "$$p(T=0| D=0) = 1-\\epsilon_{\\rm FP}$$\n",
    "$$p(T=1| D=1) = 1-\\epsilon_{\\rm FN}$$\n",
    "\n",
    "Let's assume that $\\epsilon_{\\rm FP}=0.02$ and $\\epsilon_{\\rm FN}=0.001$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In graphical form this 2x2 p(T=0 or 1|D=0 or 1) matrix is:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"http://www.astroml.org/_images/fig_contingency_table_1.png\" width=500 align>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we have **prior knowledge** regarding how likely the disease is, we can take this into account.\n",
    "\n",
    "$$p(D=1)=\\epsilon_D$$\n",
    "\n",
    "and then $p(D=0)=1-\\epsilon_D$. Say, $\\epsilon_D = 0.01$. \n",
    "\n",
    "Now assume that a person tested positive. What is the probability that this person has the disease? Is it 98% \n",
    "because $\\epsilon_{\\rm FP}=0.02$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can't just read $p(D=1|T=1)$ off the table because the table entry is the conditional probability of the *test* given the *data*, $p(T=1|D=1)$. What we want is the conditional probability of the *data* given the *test*, that is, $p(D=1|T=1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayes' rule then can be used to help us determine how likely it is that you have the disease if you tested positive:\n",
    "\n",
    "$$p(D=1|T=1) = \\frac{p(T=1|D=1)p(D=1)}{p(T=1)},$$\n",
    "\n",
    "where $$p(T=1) = p(T=1|D=0)p(D=0) + p(T=1|D=1)p(D=1).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore:\n",
    "\n",
    "$$p(D=1|T=1) = \\frac{(1 - \\epsilon_{FN})\\epsilon_D}{\\epsilon_{FP}(1-\\epsilon_D) + (1-\\epsilon_{FN})\\epsilon_D} \\approx \\frac{\\epsilon_D}{\\epsilon_{FP}+\\epsilon_D}$$\n",
    "\n",
    "That means that to get a reliable diagnosis, we need $\\epsilon_{FP}$ to be quite small.  (Because you *want* the probability to be close to unity if you test positive, otherwise it is a *false* positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our example, we have a disease rate of 1% ($\\epsilon_D = 0.01$) and a false positive rate of 2% ($\\epsilon_{\\rm FP}=0.02$).  \n",
    "\n",
    "So we have:\n",
    "$$p(D=1|T=1) = \\frac{0.01}{0.02+0.01} = 0.333$$\n",
    "\n",
    "Then in a sample of, e.g.,  1000 people, 10 people test positive and *actually* have the disease $(1000*0.01)$, but another 20 $(1000*0.02)$ will test positive while healthy!\n",
    "\n",
    "Therefore, in that sample of 30 people who tested positive, only 1/3 has the disease (not 98%!). \n",
    "\n",
    "Same math, with often surprising results, applies to DNA tests of murder suspects... (more about this later in the course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bottom line: Why is Bayes' rule important?\n",
    "\n",
    "Because it tells us how to combine new information (result of measurement) with our prior knowledge, resulting in an updated \"posterior\" knowledge."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
